{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS110 Final Project - Churn Prediction Application (https://github.com/snehankekre/ANN_Churn)\n",
    "   **Snehan Kekre** \n",
    "\n",
    "   **Minerva Schools at KGI**\n",
    "\n",
    "\n",
    "## Brief description of the dataset \n",
    "\n",
    "**Dataset Location:** https://www.sgi.com/tech/mlc/db/churn.all (https://archive.fo/HJx3i)\n",
    "\n",
    "\n",
    "### Application:  Predicting churn\n",
    "\n",
    "Below presented is my analysis using a supervised model to predict churn (ie. when customers cancel their plan). My application that predicts churn is of value as it is usually easier to retain current customers than to get new ones. \n",
    "\n",
    "The analyzed dataset contains the following variables:\n",
    "\n",
    "* State: the states in the U.S. (categorical)\n",
    "* Account length: the number of days that the account has been active\n",
    "* Area code: area code\n",
    "* Phone: phone number\n",
    "* Int'l Plan: Whether the customer has internal plan or not\n",
    "* VMail Plan: Whther the customer has a voicemail plan or not\n",
    "* VMail Message: number of voice mail messages\n",
    "* Day Mins: number of minutes the customer spoke per day\n",
    "* Day Calls: number of calls made by the customer per day\n",
    "* Day Charge: charge incurred by the customer per day\n",
    "* Eve Mins: number of minutes the customer spoke during the afternoon\n",
    "* Eve Calls: number of calls made by the customer in the afternoon\n",
    "* Eve Charge: charge incurred by the customer in the afternoon\n",
    "* Night Mins: number of minutes the customer spoke at night\n",
    "* Night Calls:  number of calls made by the customer at night\n",
    "* Night Charge: charge incurred by the customer at night\n",
    "* Intl Mins: number of minutes spent in international calls\n",
    "* Intl Calls: number of international calls made by the customer\n",
    "* Intl Charge: charged incurred by the customer for international calls\n",
    "* CustServ Calls: number of calls to customer service\n",
    "* Churn: if the customer has cancelled the plan or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing and exploring trends in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_data = pd.read_csv(\"churn.csv\", sep=',', decimal='.', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Length</th>\n",
       "      <th>Area Code</th>\n",
       "      <th>VMail Message</th>\n",
       "      <th>Day Mins</th>\n",
       "      <th>Day Calls</th>\n",
       "      <th>Day Charge</th>\n",
       "      <th>Eve Mins</th>\n",
       "      <th>Eve Calls</th>\n",
       "      <th>Eve Charge</th>\n",
       "      <th>Night Mins</th>\n",
       "      <th>Night Calls</th>\n",
       "      <th>Night Charge</th>\n",
       "      <th>Intl Mins</th>\n",
       "      <th>Intl Calls</th>\n",
       "      <th>Intl Charge</th>\n",
       "      <th>CustServ Calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>101.064806</td>\n",
       "      <td>437.182418</td>\n",
       "      <td>8.099010</td>\n",
       "      <td>179.775098</td>\n",
       "      <td>100.435644</td>\n",
       "      <td>30.562307</td>\n",
       "      <td>200.980348</td>\n",
       "      <td>100.114311</td>\n",
       "      <td>17.083540</td>\n",
       "      <td>200.872037</td>\n",
       "      <td>100.107711</td>\n",
       "      <td>9.039325</td>\n",
       "      <td>10.237294</td>\n",
       "      <td>4.479448</td>\n",
       "      <td>2.764581</td>\n",
       "      <td>1.562856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.822106</td>\n",
       "      <td>42.371290</td>\n",
       "      <td>13.688365</td>\n",
       "      <td>54.467389</td>\n",
       "      <td>20.069084</td>\n",
       "      <td>9.259435</td>\n",
       "      <td>50.713844</td>\n",
       "      <td>19.922625</td>\n",
       "      <td>4.310668</td>\n",
       "      <td>50.573847</td>\n",
       "      <td>19.568609</td>\n",
       "      <td>2.275873</td>\n",
       "      <td>2.791840</td>\n",
       "      <td>2.461214</td>\n",
       "      <td>0.753773</td>\n",
       "      <td>1.315491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.700000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>24.430000</td>\n",
       "      <td>166.600000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>14.160000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.400000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>201.400000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>201.200000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.050000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>216.400000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>36.790000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>10.590000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>243.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>350.800000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>59.640000</td>\n",
       "      <td>363.700000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>30.910000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>17.770000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Account Length    Area Code  VMail Message     Day Mins    Day Calls  \\\n",
       "count     3333.000000  3333.000000    3333.000000  3333.000000  3333.000000   \n",
       "mean       101.064806   437.182418       8.099010   179.775098   100.435644   \n",
       "std         39.822106    42.371290      13.688365    54.467389    20.069084   \n",
       "min          1.000000   408.000000       0.000000     0.000000     0.000000   \n",
       "25%         74.000000   408.000000       0.000000   143.700000    87.000000   \n",
       "50%        101.000000   415.000000       0.000000   179.400000   101.000000   \n",
       "75%        127.000000   510.000000      20.000000   216.400000   114.000000   \n",
       "max        243.000000   510.000000      51.000000   350.800000   165.000000   \n",
       "\n",
       "        Day Charge     Eve Mins    Eve Calls   Eve Charge   Night Mins  \\\n",
       "count  3333.000000  3333.000000  3333.000000  3333.000000  3333.000000   \n",
       "mean     30.562307   200.980348   100.114311    17.083540   200.872037   \n",
       "std       9.259435    50.713844    19.922625     4.310668    50.573847   \n",
       "min       0.000000     0.000000     0.000000     0.000000    23.200000   \n",
       "25%      24.430000   166.600000    87.000000    14.160000   167.000000   \n",
       "50%      30.500000   201.400000   100.000000    17.120000   201.200000   \n",
       "75%      36.790000   235.300000   114.000000    20.000000   235.300000   \n",
       "max      59.640000   363.700000   170.000000    30.910000   395.000000   \n",
       "\n",
       "       Night Calls  Night Charge    Intl Mins   Intl Calls  Intl Charge  \\\n",
       "count  3333.000000   3333.000000  3333.000000  3333.000000  3333.000000   \n",
       "mean    100.107711      9.039325    10.237294     4.479448     2.764581   \n",
       "std      19.568609      2.275873     2.791840     2.461214     0.753773   \n",
       "min      33.000000      1.040000     0.000000     0.000000     0.000000   \n",
       "25%      87.000000      7.520000     8.500000     3.000000     2.300000   \n",
       "50%     100.000000      9.050000    10.300000     4.000000     2.780000   \n",
       "75%     113.000000     10.590000    12.100000     6.000000     3.270000   \n",
       "max     175.000000     17.770000    20.000000    20.000000     5.400000   \n",
       "\n",
       "       CustServ Calls  \n",
       "count     3333.000000  \n",
       "mean         1.562856  \n",
       "std          1.315491  \n",
       "min          0.000000  \n",
       "25%          1.000000  \n",
       "50%          1.000000  \n",
       "75%          2.000000  \n",
       "max          9.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading and transforming fields of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filename):\n",
    "    # List the columns that are not of interest, besides the target\n",
    "    drop_cols = ['State','Area Code','Phone','Churn?']\n",
    "    \n",
    "    # List the categorical columns\n",
    "    yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "    \n",
    "    # Load the dataset\n",
    "    churn_data = pd.read_csv(filename, sep=',', decimal='.', header=0)\n",
    "    \n",
    "    # Convert the categorical columns to boolean\n",
    "    churn_data[yes_no_cols] = churn_data[yes_no_cols] == 'yes'\n",
    "    \n",
    "    # Isolate the target\n",
    "    y = churn_data['Churn?'] \n",
    "    \n",
    "    # Remove the listed columns\n",
    "    churn_data = churn_data.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # Isolate the names of the columns\n",
    "    feature_names = churn_data.columns\n",
    "    \n",
    "    # Convert the dataset to an array\n",
    "    X = churn_data.as_matrix().astype(np.float32)\n",
    "    \n",
    "    # Apply one-hot encoding to the target variable\n",
    "    y_one_hot = pd.get_dummies(y)\n",
    "    \n",
    "    # Covert the target to an array\n",
    "    y = y_one_hot.as_matrix().astype(np.float32)\n",
    "    \n",
    "    print(\"X.shape\")\n",
    "    print(X.shape)\n",
    "    print(\"y.shape\")\n",
    "    print(y.shape)\n",
    "    \n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Separate in to Test and Train data + Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_and_split(X, y):\n",
    "    \n",
    "    # Split the dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=2)\n",
    "    \n",
    "    # Standardize the variables\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    print(X_test.shape)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedfoward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of feedforward_matrix](https://i.imgur.com/diNer3g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(tf_dataset, tf_labels, tf_dropout_rate):\n",
    "    # Input layer    \n",
    "    with tf.name_scope(\"hidden_layer1\"):\n",
    "        \n",
    "        # Define the weights matrix\n",
    "        weights = tf.Variable(tf.truncated_normal([17, num_hidden_units_1]))\n",
    "        \n",
    "        # Define the bias matrix\n",
    "        biases = tf.Variable(tf.zeros([num_hidden_units_1]), name=\"biases\")\n",
    "        \n",
    "        # Define the net entries of the network\n",
    "        h1_net = tf.matmul(tf_dataset, weights) + biases\n",
    "        \n",
    "        # Define the activation function\n",
    "        h1_activ = tf.nn.relu(h1_net)\n",
    "        \n",
    "        h1_reg = tf.nn.l2_loss(weights)\n",
    "    \n",
    "    # Output layer\n",
    "    with tf.name_scope(\"output_layer\"):\n",
    "        \n",
    "        weights = tf.Variable(tf.truncated_normal([num_hidden_units_1, num_labels]))\n",
    "        \n",
    "        biases = tf.Variable(tf.zeros([num_labels]), name=\"biases\")\n",
    "        \n",
    "        out_net = tf.matmul(h1_activ, weights) + biases\n",
    "        \n",
    "        out_reg = tf.nn.l2_loss(weights)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_net, labels=tf_labels))\n",
    "    \n",
    "    loss = loss + l2_reg_param * (h1_reg + out_reg)\n",
    "    \n",
    "    return out_net, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_hidden_units_1 = 50\n",
    "num_hidden_units_2 = 30\n",
    "l2_reg_param = 0.5e-3 # Scale the loss on output and inner layers\n",
    "learning_rate = 1.5 \n",
    "num_steps = 2000 # Number of steps is inversely proportional to batch size\n",
    "num_labels = 2\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the accuracy of the method\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_ann(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        \n",
    "        tf_dataset = tf.placeholder(tf.float32, shape=(None, X_train.shape[1]))\n",
    "        \n",
    "        tf_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "        \n",
    "        tf_dropout_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        print(\"DataSet tf\")\n",
    "        \n",
    "        print(tf_dataset.get_shape()[1])\n",
    "        \n",
    "        logits, loss = forward_prop(tf_dataset, tf_labels, tf_dropout_rate)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        print(\"Initialized\")\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            \n",
    "            offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "            \n",
    "            # Generate a minibatch.\n",
    "            batch_data = X_train[offset:(offset + batch_size), :]\n",
    "            \n",
    "            batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "            \n",
    "            feed_dict = {tf_dataset : batch_data, tf_labels : batch_labels, tf_dropout_rate: dropout_rate}\n",
    "            \n",
    "            _, l, predictions = session.run(\n",
    "            [optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "            \n",
    "            if (step % 100 == 0):\n",
    "                \n",
    "                idx = np.random.permutation(batch_size)\n",
    "                \n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                \n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\n",
    "        #idx = np.random.permutation(batch_size)\n",
    "        test_pred = session.run(prediction, feed_dict={tf_dataset: X_test, tf_dropout_rate: dropout_rate})\n",
    "        print(\"\\n\\nTest accuracy: %.1f%%\" % accuracy(test_pred, y_test))\n",
    "        #correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "        y_hat = tf.argmax(test_pred,1).eval()\n",
    "        y_true = tf.argmax(y_test,1).eval()\n",
    "\n",
    "        print('Y hat\\n',y_hat)\n",
    "        print('Y True\\n',y_true)\n",
    "\n",
    "        print(metrics.classification_report(y_true, y_hat))\n",
    "        print('AUC score: ', metrics.roc_auc_score(y_true, y_hat))\n",
    "        print(\"Accuracy: %f\" % metrics.accuracy_score(y_true, y_hat))\n",
    "        cm = metrics.confusion_matrix(y_true, y_hat)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.matshow(cm)\n",
    "        #plt.title('Confusion Matrix',size=10)\n",
    "        ax.set_xticklabels([''] + ['no churn', 'churn'], size=10)\n",
    "        ax.set_yticklabels([''] + ['no churn', 'churn'], size=10)\n",
    "        #plt.ylabel('Prediction',size=10)\n",
    "        plt.xlabel('Real',size=10)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "        fig.set_size_inches(4,4)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    X, y, feature_names = read_dataset('churn.csv')\n",
    "    X_train, X_test, y_train, y_test = scale_and_split(X, y)\n",
    "    run_ann(X_train, X_test, y_train, y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.shape\n",
    "(3333, 17)\n",
    "y.shape\n",
    "(3333, 2)\n",
    "(2666, 17)\n",
    "(667, 17)\n",
    "DataSet tf\n",
    "17\n",
    "WARNING:tensorflow:From <ipython-input-17-85ca0fd19645>:25: initialize_all_variables (from tensorflow.python.ops.\n",
    "                                                                                      variables) is deprecated and \n",
    "                                                                                        will be removed)\n",
    "Instructions for updating:\n",
    "Use `tf.global_variables_initializer` instead.\n",
    "Initialized\n",
    "Minibatch loss at step 0: 9.642170\n",
    "Minibatch accuracy: 36.0%\n",
    "Minibatch loss at step 100: 0.496644\n",
    "Minibatch accuracy: 80.0%\n",
    "Minibatch loss at step 200: 0.475079\n",
    "Minibatch accuracy: 86.0%\n",
    "Minibatch loss at step 300: 0.247336\n",
    "Minibatch accuracy: 88.0%\n",
    "Minibatch loss at step 400: 0.261087\n",
    "Minibatch accuracy: 88.0%\n",
    "Minibatch loss at step 500: 0.305205\n",
    "Minibatch accuracy: 92.0%\n",
    "Minibatch loss at step 600: 0.269468\n",
    "Minibatch accuracy: 94.0%\n",
    "Minibatch loss at step 700: 0.165396\n",
    "Minibatch accuracy: 96.0%\n",
    "Minibatch loss at step 800: 0.199834\n",
    "Minibatch accuracy: 94.0%\n",
    "Minibatch loss at step 900: 0.107609\n",
    "Minibatch accuracy: 100.0%\n",
    "Minibatch loss at step 1000: 0.296621\n",
    "Minibatch accuracy: 92.0%\n",
    "Minibatch loss at step 1100: 0.145511\n",
    "Minibatch accuracy: 96.0%\n",
    "Minibatch loss at step 1200: 0.161333\n",
    "Minibatch accuracy: 96.0%\n",
    "Minibatch loss at step 1300: 0.182208\n",
    "Minibatch accuracy: 96.0%\n",
    "Minibatch loss at step 1400: 0.225527\n",
    "Minibatch accuracy: 94.0%\n",
    "Minibatch loss at step 1500: 0.219653\n",
    "Minibatch accuracy: 94.0%\n",
    "Minibatch loss at step 1600: 0.066998\n",
    "Minibatch accuracy: 100.0%\n",
    "Minibatch loss at step 1700: 0.309811\n",
    "Minibatch accuracy: 88.0%\n",
    "Minibatch loss at step 1800: 0.063317\n",
    "Minibatch accuracy: 100.0%\n",
    "Minibatch loss at step 1900: 0.101592\n",
    "Minibatch accuracy: 98.0%\n",
    "\n",
    "\n",
    "Test accuracy: 92.7%\n",
    "Y hat\n",
    " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
    " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
    " 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
    " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
    " 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
    " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
    " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
    " 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
    " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
    " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
    " 0]\n",
    "Y True\n",
    " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
    " 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0\n",
    " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
    " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
    " 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
    " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
    " 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
    " 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
    " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
    " 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
    " 0]\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.94      0.98      0.96       571\n",
    "          1       0.82      0.62      0.71        96\n",
    "\n",
    "avg / total       0.92      0.93      0.92       667\n",
    "\n",
    "AUC score:  0.801116462347\n",
    "Accuracy: 0.926537"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Confusion Matrix](https://i.imgur.com/onx2ENg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and comments\n",
    "\n",
    "I used TensorFlow in this application for its portability (flexible architecture allows for the deployment of computation to one or more CPUs or GPUs), but more importantly for the capability of auto-differention. \n",
    "\n",
    "Tensorflow keeps separate the definition of computations and their executions by first creating a graph and then using a session to carry out operations in it: ![Image of Data Flow Graph](https://i.imgur.com/QMnMGX1.jpg)\n",
    "\n",
    "Variables, constants, and operators are the nodes in the **Data Flow Graph**, while tensors (n-dimensional matrix) are edges. For greater ease in computation and lower load on hardware, it is possible to split graphs into smaller subgraphs and parallely run them across multiple cores (distributed computation). By fixing the underlying graph and activation function, the network is parameterized by a weight vector **w** belonging to **R^d**. We wish to learn the vector **w**.\n",
    "\n",
    "A benefit associated with the above implementation is that it avoids the computational complexity of **O(n^3)** from the Normal Equation associated with inverting **X^T**, where **X** is an ** *n x n* **matrix and ** *n* ** is the number of features. But using a linear regression model post training confers faster prediction as the instances and the number of features is linear w.r.t to the computational complexity. \n",
    "\n",
    "I initially wanted to implement ** *Batch Gradient Descent Optimization* ** where the gradient of the cost function is computed for each parameter. An adavantage of this method is that gradients can be computed all at once using the following equation: ![Image of Gradient vector of the cost function](https://i.imgur.com/aR0Q4NQ.jpg)\n",
    "\n",
    "For our sizable training set it is computationaly very expensive to use Batch Graident Descent as it uses the entire training data at every step. Another option was to use Stochastic Gradient Descent (SGD) which is faster as it picks a random point in the training set and computes the gradients relying only on the one point. However, the cost function will decrease only on average due to the method's stochastic nature. Consequently, the values of our output parameters are not optimal. This can be mitigated by gradually tuning the learning rate by simulated annealing, but the extra cost associated with this makes it undesirable. I instead used ** *Mini-batch Gradient Descent Optimization* ** because unlike batch gradient which computed gradients based on the entire training set, and unlike SGD which did it based on one point, ** *Mini-batch Gradient Descent* ** provides a performance boost and lowers computational cost by compting gradients on subsets or **mini-batches** of the training set and thus eases in **tensor** operations.\n",
    "\n",
    "In computing the loss, additional time complexity is added due to the computationally expensive (computing the exponential of every score and then normalizing them) Softmax function:![Image of Softmax function](https://i.imgur.com/02UPYxW.jpg)To minimize the cost function, cross entropy is measured to see how well estimated probablities match the target class of probablities (*tf.nn.softmax_cross_entropy_with_logits*). \n",
    "\n",
    "The aforementioned gradients are computed by taking the derivate of y w.r.t. each tensor in the list. TensorFlow allows for automatic differention computed on a graph: ![Image of Gradients computed for a graph](https://i.imgur.com/3pdWiyI.jpg)\n",
    "\n",
    "To reduce the likelihood of a vanishing gradient my choice of ativation function was ReLU instead of a sigmoid function or softplus.\n",
    "## Rectified linear function\n",
    "\n",
    "![Image of Softplus_vs_Rectifier](https://i.imgur.com/y7WFOOC.png)\n",
    "\n",
    "The scaling behavior of backpropgagtion in this example can be understood through its time complexity. In the case of ** *x* ** training points, ** *y* ** features, ** *z* ** hidden layes (** *z* ** = 0 here), with each ** *n* ** neurons and ** *o* ** output neurons and ** *i* ** iterations, the time complexity is ** *O*(*x*.*y*.(*n* ^*z*).*o*.*i*) **\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
